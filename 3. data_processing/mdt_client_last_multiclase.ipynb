{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d96b9d34-c82b-4973-8a9e-67cb24767fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Librerias y Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c462651e-31ea-41e6-a25c-d0cd6f12a039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", True)\n",
    "spark.conf.set('spark.sql.shuffle.partitions', 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f3863d50-8884-42f4-8508-d55d49d44901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "### Librerías"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "### Funciones Ingenieria de datos"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "### Funciones de ingesta en RDS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "### Funciones de control de flujo de ingesta"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../../../../../04_utils/commons_functions_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dfdcc343-3dbd-468a-9906-64fc8f5baf39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../../../../04_utils/commons_functions_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249d9e17-27d2-4085-92b5-96a4c9b3e9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../../../../spigot/initial/global_parameter_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4e562ca1-6e27-40c4-9aff-043bb0b523b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# mute warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from pyspark.sql import Window\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import plotly.express as px\n",
    "\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "from datetime import datetime, date\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2f10973-bdc0-4645-bf65-f5facdd0a555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Carga de Fuente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "236bf082-34d4-4f72-949c-f8e00e1de5d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "int_pedidos_clientes = (spark.read.parquet(\"/Volumes/dbw_prod_aavanzada/db_tmp/files/pburbano/data/\")\n",
    "                                  .withColumn(\"fecha_pedido_dt\", F.to_date(F.col(\"fecha_pedido_dt\")))\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e1966ca-124d-42b0-906e-0f1d2201ae95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Creacion de MDT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c18eb71-89bb-4c0d-99ac-5617e5ffb0c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# === 1. Definir ventanas ===\n",
    "window_cliente = Window.partitionBy(\"cliente_id\").orderBy(\"fecha_pedido_dt\")\n",
    "\n",
    "# === 2. Enriquecer DF base ===\n",
    "df = int_pedidos_clientes.withColumn(\"fecha_ultimo_pedido\", F.max(\"fecha_pedido_dt\").over(Window.partitionBy(\"cliente_id\")))\n",
    "df = df.withColumn(\"canal_previo\", F.lag(\"canal_pedido_cd\").over(window_cliente))\n",
    "df = df.withColumn(\"es_ultimo\", F.when(F.col(\"fecha_pedido_dt\") == F.col(\"fecha_ultimo_pedido\"), 1).otherwise(0))\n",
    "\n",
    "# === 3. Crear target multiclase (sin filtrar unicidad de pedidos en la última fecha) ===\n",
    "df = df.withColumn(\n",
    "    \"target\",\n",
    "    F.when((F.col(\"canal_previo\") != \"DIGITAL\") & (F.col(\"canal_pedido_cd\") != \"DIGITAL\"), 0)\n",
    "     .when((F.col(\"canal_previo\") == \"DIGITAL\") & (F.col(\"canal_pedido_cd\") == \"DIGITAL\"), 1)\n",
    "     .when((F.col(\"canal_previo\") != \"DIGITAL\") & (F.col(\"canal_pedido_cd\") == \"DIGITAL\"), 2)\n",
    "     .when((F.col(\"canal_previo\") == \"DIGITAL\") & (F.col(\"canal_pedido_cd\") != \"DIGITAL\"), 3)\n",
    ")\n",
    "\n",
    "# === 4. Filtrar último pedido con target válido ===\n",
    "df_target = (\n",
    "    df.filter(\"es_ultimo = 1\")\n",
    "      .filter(F.col(\"target\").isNotNull())\n",
    "      .select(\"cliente_id\", \"target\", F.col(\"fecha_ultimo_pedido\").alias(\"fecha_ultimo_pedido_target\"))\n",
    ")\n",
    "\n",
    "# === 5. Crear histórico previo al último pedido ===\n",
    "df_historico = (\n",
    "    df.filter(F.col(\"es_ultimo\") == 0)\n",
    "      .join(df_target.select(\"cliente_id\", \"fecha_ultimo_pedido_target\"), \"cliente_id\", \"inner\")\n",
    "      .withColumn(\"dias_antes_ultimo\", F.datediff(\"fecha_ultimo_pedido_target\", \"fecha_pedido_dt\"))\n",
    "      .withColumn(\"canal_pedido_cd\", F.when(F.col(\"canal_pedido_cd\") == \"DIGITAL\", \"DIGITAL\").otherwise(\"NO_DIGITAL\"))\n",
    ")\n",
    "\n",
    "df_historico = df_historico.repartition(\"cliente_id\").persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# === 6. Variables agregadas del histórico ===\n",
    "# Pedidos y frecuencia\n",
    "f_pedidos = df_historico.groupBy(\"cliente_id\").agg(\n",
    "    F.count(\"*\").alias(\"n_pedidos_previos\"),\n",
    "    F.countDistinct(\"canal_pedido_cd\").alias(\"n_canales_utilizados\")\n",
    ")\n",
    "\n",
    "# calcular días entre pedidos\n",
    "w_orden = Window.partitionBy(\"cliente_id\").orderBy(\"fecha_pedido_dt\")\n",
    "df_historico = df_historico.withColumn(\n",
    "    \"dias_entre_pedidos\", F.datediff(\"fecha_pedido_dt\", F.lag(\"fecha_pedido_dt\").over(w_orden))\n",
    ")\n",
    "\n",
    "# agregar canal previo y cambio de canal\n",
    "df_historico = df_historico.withColumn(\"canal_previo\", F.lag(\"canal_pedido_cd\").over(w_orden))\n",
    "df_historico = df_historico.withColumn(\n",
    "    \"cambio_canal\", F.when(F.col(\"canal_previo\") != F.col(\"canal_pedido_cd\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# cambios hacia/desde digital\n",
    "df_historico = df_historico.withColumn(\n",
    "    \"cambio_a_digital\",\n",
    "    F.when((F.col(\"canal_previo\") != \"DIGITAL\") & (F.col(\"canal_pedido_cd\") == \"DIGITAL\"), 1).otherwise(0)\n",
    ")\n",
    "df_historico = df_historico.withColumn(\n",
    "    \"cambio_desde_digital\",\n",
    "    F.when((F.col(\"canal_previo\") == \"DIGITAL\") & (F.col(\"canal_pedido_cd\") != \"DIGITAL\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# frecuencia de pedidos\n",
    "f_frecuencia = df_historico.groupBy(\"cliente_id\").agg(\n",
    "    F.mean(\"dias_entre_pedidos\").alias(\"dias_entre_pedidos_mean\"),\n",
    "    F.expr(\"percentile(dias_entre_pedidos, 0.5)\").alias(\"dias_entre_pedidos_median\"),\n",
    "    F.min(\"dias_entre_pedidos\").alias(\"dias_entre_pedidos_min\"),\n",
    "    F.max(\"dias_entre_pedidos\").alias(\"dias_entre_pedidos_max\")\n",
    ")\n",
    "\n",
    "# Conteo de cada canal\n",
    "f_canales = df_historico.groupBy(\"cliente_id\").pivot(\"canal_pedido_cd\").count().fillna(0)\n",
    "if \"DIGITAL\" not in f_canales.columns:\n",
    "    f_canales = f_canales.withColumn(\"DIGITAL\", F.lit(0))\n",
    "if \"NO_DIGITAL\" not in f_canales.columns:\n",
    "    f_canales = f_canales.withColumn(\"NO_DIGITAL\", F.lit(0))\n",
    "f_canales = (\n",
    "    f_canales.withColumnRenamed(\"DIGITAL\", \"n_digital\")\n",
    "             .withColumnRenamed(\"NO_DIGITAL\", \"n_no_digital\")\n",
    "             .withColumn(\n",
    "                 \"prop_digital\",\n",
    "                 F.when(\n",
    "                     (F.col(\"n_digital\") + F.col(\"n_no_digital\")) > 0,\n",
    "                     F.col(\"n_digital\") / (F.col(\"n_digital\") + F.col(\"n_no_digital\"))\n",
    "                 ).otherwise(0)\n",
    "             )\n",
    ")\n",
    "\n",
    "# variables numericas\n",
    "f_valores = (\n",
    "    df_historico.groupBy(\"cliente_id\").agg(\n",
    "        F.sum(\"facturacion_usd_val\").alias(\"facturacion_total\"),\n",
    "        F.avg(\"facturacion_usd_val\").alias(\"facturacion_prom\"),\n",
    "        F.stddev(\"facturacion_usd_val\").alias(\"facturacion_std\"),\n",
    "        F.sum(\"materiales_distintos_val\").alias(\"materiales_distintos_total\"),\n",
    "        F.avg(\"materiales_distintos_val\").alias(\"materiales_prom\"),\n",
    "        F.stddev(\"materiales_distintos_val\").alias(\"materiales_std\"),\n",
    "        F.sum(\"cajas_fisicas\").alias(\"cajas_fisicas_total\"),\n",
    "        F.avg(\"cajas_fisicas\").alias(\"cajas_fisicas_prom\"),\n",
    "        F.stddev(\"cajas_fisicas\").alias(\"cajas_fisicas_std\"),\n",
    "    )\n",
    "    .fillna(0, subset=[\"facturacion_std\", \"materiales_std\", \"cajas_fisicas_std\"])\n",
    ")\n",
    "\n",
    "# variables que no cambian en el tiempo\n",
    "f_variables_fijas = (\n",
    "    df_historico.groupBy(\"cliente_id\").agg(\n",
    "        F.first(\"pais_cd\").alias(\"pais_cd\"),\n",
    "        F.first(\"region_comercial_txt\").alias(\"region_comercial_txt\"),\n",
    "        F.first(\"tipo_cliente_cd\").alias(\"tipo_cliente_cd\"),\n",
    "        F.first(\"madurez_digital_cd\").alias(\"madurez_digital_val\"),\n",
    "        F.first(\"estrellas_txt\").cast(\"int\").alias(\"estrellas_val\"),\n",
    "        F.length(F.first(\"frecuencia_visitas_cd\")).alias(\"frecuencia_visitas_val\"),\n",
    "        F.first(\"fecha_ultimo_pedido_target\").cast(\"timestamp\").alias(\"fecha_ultimo_pedido\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# informacion de agencia y ruta\n",
    "df_historico = (\n",
    "    df_historico.withColumn(\"agencia_num\", regexp_extract(F.col(\"agencia_id\"), r\"A(\\d+)$\", 1).cast(\"int\"))\n",
    "                 .withColumn(\"ruta_num\", regexp_extract(F.col(\"ruta_id\"), r\"R(\\d+)$\", 1).cast(\"int\"))\n",
    ")\n",
    "\n",
    "f_ruta_agencia = (\n",
    "    df_historico.groupBy(\"cliente_id\").agg(\n",
    "        F.first(\"agencia_num\").alias(\"agencia_num\"),\n",
    "        F.first(\"ruta_num\").alias(\"ruta_num\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# === MDT FINAL ===\n",
    "df_mdt = (\n",
    "    f_pedidos\n",
    "    .join(f_canales, \"cliente_id\", \"left\")\n",
    "    .join(f_valores, \"cliente_id\", \"left\")\n",
    "    .join(f_frecuencia, \"cliente_id\", \"left\")\n",
    "    .join(f_ruta_agencia, \"cliente_id\", \"left\")\n",
    "    .join(f_variables_fijas, \"cliente_id\", \"left\")\n",
    "    .join(df_target.select(\"cliente_id\", \"target\"), \"cliente_id\", \"inner\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6602459453049726,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "mdt_client_last_multiclase",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}